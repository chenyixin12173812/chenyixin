# java集合问题

# 1 1.7和1.8版本的HashMap有什么区别？1.7的有什么隐患，什么原因导致的？ 

# 2  跟HashTable，ConcurrentHashMap有什么区别？ 

# 3 为什么要转成红黑树呢？

 所以只有当包含足够多的 Nodes 时才会转成 TreeNodes，这个足够多的标准就是由 TREEIFY_THRESHOLD 的值（默认值8）决定的。而当桶中节点数由于移除或者 resize (扩容) 变少后，红黑树会转变为普通的链表，这个阈值是 UNTREEIFY_THRESHOLD（默认值6）  

红黑树虽然查询效率比链表高，但是结点占用的空间大，只有达到一定的数目才有树化的意义，这是**基于时间和空间的平衡考虑**。 

# 4 **为什么树化标准是8个**，退化是6个

```
* 0:    0.60653066
* 1:    0.30326533
* 2:    0.07581633
* 3:    0.01263606
* 4:    0.00157952
* 5:    0.00015795
* 6:    0.00001316
* 7:    0.00000094
* 8:    0.00000006
```

 注释中给我们展示了1-8长度的具体命中概率，当长度为8的时候，概率概率仅为0.00000006，这么小的概率，HashMap的红黑树转换几乎不会发生 

 主要是一个过渡，避免链表和红黑树之间频繁的转换。如果阈值是7的话，删除一个元素红黑树就必须退化为链表，增加一个元素就必须树化，来回不断的转换结构无疑会降低性能，所以阈值才不设置的那么临界 

![1597160986874](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\1597160986874.png)

# 5 HashMap为什么不用B+树来替换红黑树

B+胖，一层太多，用来查找的，退化成链表。

# 6 HashMap为什么为什么不使用AVL树而使用红黑树？

红黑树和AVL树都是**最常用的平衡二叉搜索树**，它们的查找、删除、修改都是O(lgn) time

AVL树和红黑树有几点比较和区别：
（1）AVL树是更加严格的平衡，因此可以提供更快的查找速度，一般读取查找密集型任务，适用AVL树。
（2）红黑树更适合于插入修改密集型任务。
（3）通常，AVL树的旋转比红黑树的旋转更加难以平衡和调试。

**总结**：
（1）AVL以及红黑树是高度平衡的树数据结构。它们非常相似，真正的区别在于在任何添加/删除操作时完成的旋转操作次数。
（2）两种实现都缩放为a O(lg N)，其中N是叶子的数量，但实际上AVL树在查找密集型任务上更快：利用更好的平衡，树遍历平均更短。另一方面，插入和删除方面，AVL树速度较慢：需要更高的旋转次数才能在修改时正确地重新平衡数据结构。
（3）在AVL树中，从根到任何叶子的最短路径和最长路径之间的差异最多为1。在红黑树中，差异可以是2倍。
（4）两个都给O（log n）查找，但平衡AVL树可能需要O（log n）旋转，而红黑树将需要最多两次旋转使其达到平衡（尽管可能需要检查O（log n）节点以确定旋转的位置）。旋转本身是O（1）操作，因为你只是移动指针。

# 7 既然红黑树那么好，为啥hashmap不直接采用红黑树，而是当大于8个的时候才转换红黑树？

 因为红黑树需要进行左旋，右旋操作， 而单链表不需要，
以下都是单链表与红黑树结构对比。
如果元素小于8个，查询成本高，新增成本低
如果元素大于8个，查询成本低，新增成本高 

# 8 hashmap的实现，hashtable，concurrenthashmap实现 

# 9 HashSet 和 HashMap 区别 

### HashSet和HashMap的区别

| *HashMap*                                   | *HashSet*                                                    |
| ------------------------------------------- | ------------------------------------------------------------ |
| HashMap实现了Map接口                        | HashSet实现了Set接口                                         |
| HashMap储存键值对                           | HashSet仅仅存储对象                                          |
| 使用put()方法将元素放入map中                | 使用add()方法将元素放入set中                                 |
| HashMap中使用键对象来计算hashcode值         | HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false |
| HashMap比较快，因为是使用唯一的键来获取对象 | HashSet较HashMap来说比较慢                                   |

# 10 hashmap的问题

## 1 hashmap多线程操作同时调用put()方法后可能导致get()死循环,从而使CPU使用率达到100%,从而使服务器宕机. 

多个线程put的时候造成了某个key值Entry key List的死循环，然后再调用put方法操作的时候就会进入链表的死循环内。  

**如何产生的？**

 内部实现机制(在多线程环境且未作同步的情况下，对同一个HashMap做put操作可能导致两个或以上线程同时做rehash动作，就可能导致循环键表出现. 

## (1)正常的ReHash过程(hashmap产生死循环链表的操作)

抄了个图做个演示。

1. 我假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。
2. 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table1这里了。
3. 接下来的三个步骤是Hash表 resize成4，然后所有的 重新rehash的过程。

 

![img](https://images.cnblogs.com/cnblogs_com/andy-zhou/817145/o_HashMap001.jpg)

 

## (2)并发的Rehash过程

（1）假设我们有两个线程。我用红色和浅蓝色标注了一下。我们再回头看一下我们的 transfer代码中的这个细节：

```java
do {    Entry<K,V> next = e.next; // <--假设线程一执行到这里就被调度挂起了    int i = indexFor(e.hash, newCapacity);    e.next = newTable[i];    newTable[i] = e;    e = next;} while (e != null);
```

 而我们的线程二执行完成了。于是我们有下面的这个样子。

![img](https://images.cnblogs.com/cnblogs_com/andy-zhou/817145/o_HashMap002.jpg)

 

注意：因为Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。我们可以看到链表的顺序被反转后。

（2）线程一被调度回来执行。

1. 先是执行 newTalbe[i] = e。
2. 然后是e = next，导致了e指向了key(7)。
3. 而下一次循环的next = e.next导致了next指向了key(3)。

![img](https://images.cnblogs.com/cnblogs_com/andy-zhou/817145/o_HashMap003.jpg)

###  (3)再接下来

 线程一接着工作。把key(7)摘下来，放到newTable[i]的第一个，然后把e和next往下移。

![img](https://images.cnblogs.com/cnblogs_com/andy-zhou/817145/o_HashMap004.jpg)

###  （4）环形链接出现

e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 

![img](https://images.cnblogs.com/cnblogs_com/andy-zhou/817145/o_HashMap005.jpg)

 于是，当我们的线程一调用到，HashTable.get(11)时，悲剧就出现了——Infinite Loop



## 3 多线程put的时候可能导致元素丢失

HashMap另外一个并发可能出现的问题是，可能产生元素丢失的现象。

考虑在多线程下put操作时，执行addEntry(hash, key, value, i)，如果有产生哈希碰撞，
导致两个线程得到同样的bucketIndex去存储，就可能会出现覆盖丢失的情况：







# 2 内存泄漏

# 3 头插法和尾插法

1.JDK8以前是头插法，JDK8后是尾插法

2.为什么要从头插法改成尾插法？
A.因为头插法会造成死链，[参考链接](https://blog.csdn.net/chenyiminnanjing/article/details/82706942)
B.JDK7用头插是考虑到了一个所谓的热点数据的点(新插入的数据可能会更早用到)，但这其实是个伪命题,因为JDK7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置(就是因为头插) 所以最后的结果 还是打乱了插入的顺序 所以总的来看支撑JDK7使用头插的这点原因也不足以支撑下去了 所以就干脆换成尾插 一举多得

# 11 jdk1.7与jdk1.8中HashMap区别

\1. 最重要的一点是底层结构不一样，1.7是数组+链表，1.8则是数组+链表+红黑树结构;

\2. jdk1.7中当哈希表为空时，会先调用inflateTable()初始化一个数组；而1.8则是直接调用resize()扩容;

\3. 插入键值对的put方法的区别，1.8中会将节点插入到链表尾部，而1.7中是采用头插；

\4. jdk1.7中的hash函数对哈希值的计算直接使用key的hashCode值，而1.8中则是采用key的hashCode异或上key的hashCode进行无符号右移16位的结果，避免了只靠低位数据来计算哈希时导致的冲突，计算结果由高低位结合决定，使元素分布更均匀；
\5. 扩容时1.8会保持原链表的顺序，而1.7会颠倒链表的顺序；而且1.8是在元素插入后检测是否需要扩容，1.7则是在元素插入前；
\6. jdk1.8是扩容时通过hash&cap==0将链表分散，无需改变hash值，而1.7是通过更新hashSeed来修改hash值达到分散的目的；

\7. 扩容策略：1.7中是只要不小于阈值就直接扩容2倍；而1.8的扩容策略会更优化，当数组容量未达到64时，以2倍进行扩容，超过64之后若桶中元素个数不小于7就将链表转换为红黑树，但如果红黑树中的元素个数小于6就会还原为链表，当红黑树中元素不小于32的时候才会再次扩容。