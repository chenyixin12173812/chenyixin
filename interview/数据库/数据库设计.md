# 数据库设计

# 0 集群设计

 https://www.cnblogs.com/wintersun/p/4638176.html 

 https://www.cnblogs.com/zhangchao0515/p/11493236.html 

# 0.0 mysql支持的集群方案

### **复制的基本原理**

- slave 会从 master 读取 binlog 来进行数据同步
- 三个步骤

1. master将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events；
2. salve 将 master 的 binary log events 拷贝到它的中继日志（relay log）;
3. slave 重做中继日志中的事件，将改变应用到自己的数据库中。MySQL 复制是异步且是串行化的。

![img](https://pic3.zhimg.com/80/v2-cd12a431f29aedbc2ec6cc66ff25ca96_720w.jpg)

### **复制的基本原则**

- 每个 slave只有一个 master
- 每个 salve只能有一个唯一的服务器 ID
- 每个master可以有多个salve

1 单主多从

  1个Master复制多个 Slave 的架构实施非常简单， 多个 Slave 和单个 Slave的实施
并没有实质性的区别。 在 Master 端并不 Care 有多少个 Slave 连上了自己， 只要有 Slave
的 IO 线程通过了连接认证， 向他请求指定位置之后的 Binary Log 信息， 他就会按照该IO
线程的要求  



2 双主多从 

  有些时候，简单的从一个 MySQL 复制到另外一个 MySQL 的基本 Replication 架构，
可能还会需要在一些特定的场景下进行 Master 的切换。 如在 Master 端需要进行一些特别
的维护操作的时候，可能需要停 MySQL 的服务。这时候，为了尽可能减少应用系统写服务
的停机时间，最佳的做法就是将我们的 Slave 节点切换成 Master 来提供写入的服务。  

3 联复制架构(Master - Slaves - Slaves ...)  

  在有些应用场景中，可能读写压力差别比较大，读压力特别的大，一个 Master 可能需
要上10台甚至更多的 Slave 才能够支撑注读的压力。这时候， Master 就会比较吃力了，
因为仅仅连上来的 Slave IO 线程就比较多了， 这样写的压力稍微大一点的时候，Master 端
因为复制就会消耗较多的资源，很容易造成复制的延时  

## 0.1 58同城可用性设计 

1 .复制+冗余

副作用：复制+冗余一定会引发一致性问题

保证“读”高可用的方法：复制从库，冗余数据，如下图

 [![0](https://images0.cnblogs.com/blog/15172/201507/111044087215255.png)](http://images0.cnblogs.com/blog/15172/201507/111044078939141.png) 
带来的问题：主从不一致

2 **保证“写”高可用的一般方法：双主模式，即复制主库（很多公司用单master，此时无法保证写的可用性**），冗余数据，如下图

 [![1](https://images0.cnblogs.com/blog/15172/201507/111044100027296.png)](http://images0.cnblogs.com/blog/15172/201507/111044092683155.png) 
带来的问题：双主同步key冲突，引不一致

解决方案：

a）方案一：由数据库或者业务层保证key在两个主上不冲突

b）方案二：58同城保证“写”高可用的方法：“双主”当“主从”用，不做读写分离，在“主”挂掉的情况下，“从”（其实是另外一个主），顶上，如下图

 [![2](https://images0.cnblogs.com/blog/15172/201507/111044119248209.png)](http://images0.cnblogs.com/blog/15172/201507/111044113305324.png) 
优点：读写都到主，解决了一致性问题；“双主”当“主从”用，解决了可用性问题

**（2）读性能设计：****如何扩展读性能**

最常用的方法是，建立索引

建立非常多的索引，副作用是：

a）降低了写性能

b）索引占内存多了，放在内存中的数据就少了，数据命中率就低了，IO次数就多了

但是否想到，不同的库可以建立不同的索引呢？如下图

 [![3](https://images0.cnblogs.com/blog/15172/201507/111044136112193.png)](http://images0.cnblogs.com/blog/15172/201507/111044128613822.png) 
TIPS：**不同的库可以建立不同索引**

主库只提供写，不建立索引

online从库只提供online读，建立online读索引

offline从库只提供offline读，建立offline读索引

提高读性能常见方案二，增加从库

[![4](https://images0.cnblogs.com/blog/15172/201507/111044154396836.png)](http://images0.cnblogs.com/blog/15172/201507/111044141277336.png)

上文已经提到，这种方法会引发主从不一致问题，从库越多，主从时延越长，不一致问题越严重

这种方案很常见，但58没有采用

提高读性能方案三，增加缓存

**传统缓存的用法**是：

a）发生写请求时，先淘汰缓存，再写数据库

b）发生读请求时，先读缓存，hit则返回，miss则读数据库并将数据入缓存（此时可能旧数据入缓存），如下图

 [![5](https://images0.cnblogs.com/blog/15172/201507/111044171116590.png)](http://images0.cnblogs.com/blog/15172/201507/111044163147935.png) 
带来的问题：

a）如上文所述，数据复制会引发一致性问题，由于主从延时的存在，可能引发缓存与数据库数据不一致

b）所有app业务层都要关注缓存，无法屏蔽“主+从+缓存”的复杂性

**58同城缓存使用方案**：服务+数据+缓存

 [![6](https://images0.cnblogs.com/blog/15172/201507/111044194715904.png)](http://images0.cnblogs.com/blog/15172/201507/111044180641134.png) 
好处是：

1）引入服务层屏蔽“数据库+缓存”

2）不做读写分离，读写都到主的模式，不会引发不一致

**（3）一致性设计**

**主从不一致**解决方案

方案一：引入中间件

 [![7](https://images0.cnblogs.com/blog/15172/201507/111044217997759.png)](http://images0.cnblogs.com/blog/15172/201507/111044206438446.png) 
中间件将key上的写路由到主，在一定时间范围内（主从同步完成的经验时间），该key上的读也路由到主

方案二：读写都到主

[![8](https://images0.cnblogs.com/blog/15172/201507/111044237684658.png)](http://images0.cnblogs.com/blog/15172/201507/111044224396631.png)

上文已经提到，58同城采用了这种方法，不做读写分离，不会不一致

**数据库与缓存不一致解决方案**

两次淘汰法

[![9](https://images0.cnblogs.com/blog/15172/201507/111044253779899.png)](http://images0.cnblogs.com/blog/15172/201507/111044245028799.png)

异常的读写时序，或导致旧数据入缓存，一次淘汰不够，要进行二次淘汰

a）发生写请求时，先淘汰缓存，再写数据库，额外增加一个timer，一定时间（主从同步完成的经验时间）后再次淘汰

b）发生读请求时，先读缓存，hit则返回，miss则读数据库并将数据入缓存（此时可能旧数据入缓存，但会被二次淘汰淘汰掉，最终不会引发不一致）

**（4）扩展性设计**

（4.1）58同城秒级别数据扩容

需求：原来水平切分为N个库，现在要扩充为2N个库，希望不影响服务，在秒级别完成

 [![10](https://images0.cnblogs.com/blog/15172/201507/111044266899399.png)](http://images0.cnblogs.com/blog/15172/201507/111044261744256.png) 
最开始，分为2库，0库和1库，均采用“双主当主从用”的模式保证可用性

 [![11](https://images0.cnblogs.com/blog/15172/201507/111044279558211.png)](http://images0.cnblogs.com/blog/15172/201507/111044274244541.png) 
接下来，将从库提升，并修改服务端配置，秒级完成扩库

由于是2扩4，不会存在数据迁移，原来的0库变为0库+2库，原来的1库变为1库和3库

此时损失的是数据的可用性

 [![12](https://images0.cnblogs.com/blog/15172/201507/111044311898624.png)](http://images0.cnblogs.com/blog/15172/201507/111044295184168.png) 
最后，解除旧的双主同步（0库和2库不会数据冲突），为了保证可用性增加新的双主同步，并删除掉多余的数据

这种方案可以秒级完成N库到2N库的扩容。

存在的问题：只能完成N库扩2N库的扩容（不需要数据迁移），非通用扩容方案（例如3库扩4库就无法完成）

（4.2）非指数扩容，数据库增加字段，数据迁移

[这些方法在（上）篇中都已经介绍过，此处不再冗余，有兴趣的朋友回复“同城”回看（上）篇]

方案一：追日志方案

方案二：双写方案

## 0.2 读写分离方案
方案1：应用程序根据业务逻辑来判断，增删改等写操作命令发给主库，查询命令发给备库。
特点：数据库和应用程序强耦合，数据库如果有变化还好影响主库。<2>应用程序复杂化。

方案2：利用中间件来做代理，负责对数据库的请求识别出读还是写，并分发到不同的数据库中。
特点：<1> 数据库和应用程序弱耦合。<2> 代理存在性能瓶颈和可靠性风险增加，相对可控。
常见的中间件：<1> MySQL-Proxy 

 <2> Amoeba for MySQL(原文推荐这个，没机会对比过)  

<3> mycat  

<4> DBProxy  

<5> 公有云的RDS数据库+数据库中间件，如华为云的RDS(关系型数据库)+DDM(分布式数据库中间件)

## 0.3 负载均衡

## 0.4 集群可用性探测机制

## 0.5 提供分片规则和路由策略

# 0.6 集群技术对比

 https://www.csdn.net/article/2009-06-16/2503 

**Oracle’s Real Application Cluster (我们将称之为RAC). 
Microsoft SQL Cluster Server (我们将称之为MSCS)..IBM’s [DB2](http://www.ibm.com/developerworks/cn/db2/zones/db2ii/index.html) UDB High Availability Cluster (我们将称之为UDB) Sybase ASE High Availability Cluster (我们将称之为ASE) MySQL High Availability Cluster (我们将称之为MySQL CS).Parallel Computers Technology Inc.’s ICX-UDS middleware 我们将称之为ICX). 除了ICX，所有其它的集群技术都是基于数据库引擎的。所以ICX可以支持任何当前流行的数据库**

# 1.数据库表设计

 https://blog.csdn.net/weixin_34290631/article/details/92651556?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param 

# 2 数据分片

我们的系统是否需要分库分表吗？

一般MySQL单表1000W左右的数据是没有问题的（前提是应用系统和数据库等层面设计和优化的比较好）。当然，除了考虑当前的数据量和性能情况时，作为架构师，我们需要提前考虑系统半年到一年左右的业务增长情况，对数据库服务器的QPS、连接数、容量等做合理评估和规划，并提前做好相应的准备工作。如果单机无法满足，且很难再从其他方面优化，那么说明是需要考虑分片的。这种情况可以先去掉数据库中自增ID，为分片和后面的数据迁移工作提前做准备。

很多人觉得“分库分表”是宜早不宜迟，应该尽早进行。

## 2.1 整体方案

### 2.1.0 水平分表、水平分库、垂直分库分表

垂直分库：把不同的表分到不同的数据库里。

垂直分表：把一张表的不同字段，放在不同的数据库，冷热隔离。

水平分库: 把一个大表拆分成多个小表（user分为user_1， user_2），不在同一个数据库内

水平分表:把一个大表拆分成多个小表（user分为user_1， user_2），在同一个数据库内。

### 2.1.1水平切分，到底是分库还是分表？

强烈建议分库，而不是分表，因为：

- 分表依然公用一个数据库文件，仍然有磁盘IO的竞争
- 分库能够很容易的将数据迁移到不同数据库实例，甚至数据库机器上，扩展性更好

### 2.1.2 为什么不分区？

所有数据逻辑上还在一个表中，但物理上，可以根据一定的规则放在不同的文件中。这是MySQL5.1之后支持的功能，业务代码无需改动。

**分区表看上去很帅气，为什么大部分互联网公司不使用，而更多的选择分库分表来进行水平切分呢？**

分区表的一些缺点，是大数据量，高并发量的业务难以接受的：

（1）如果SQL不走分区键，很容易出现全表锁；

（2）在分区表实施关联查询，就是一个灾难；

（3）分库分表，自己掌控业务场景与访问模式，可控；分区表，工程师写了一个SQL，自己无法确定MySQL是怎么玩的，不可控；

*画外音：类似于，不要把业务逻辑实现在存储过程，用户自定义函数，触发器里，而要实现在业务代码里一样。*

（4）DBA给OP埋坑，容易大打出手，造成同事矛盾；

（5）…

当然，在数据量和并发量不太大，或者按照时间来存储冷热数据或归档数据的一些特定场景下，分区表还是有上场机会的

## 2.2 分片规则和策略

### 2.2 .1分片字段该如何选择

在开始分片之前，我们首先要确定分片字段（也可称为“片键”）。很多常见的例子和场景中是采用ID或者时间字段进行拆分。这也并不绝对的，我的建议是结合实际业务，通过对系统中执行的sql语句进行统计分析，选择出需要分片的那个表中最频繁被使用，或者最重要的字段来作为分片字段。

### 2.2 .2常见分片规则

常见的分片策略有**随机分片**和**连续分片**这两种，如下图所示：

 ![æ°´å¹³ååºåè¡¨çå³é®æ­¥éª¤ä»¥åå¯è½éå°çé®é¢](https://static001.infoq.cn/resource/image/b7/45/b7be0f4a8b7ca71657ade77253a3ba45.png) 

1 当需要使用分片字段进行范围查找时，连续分片可以快速定位分片进行高效查询，大多数情况下可以有效避免跨分片查询的问题。后期如果想对整个分片集群扩容时，只需要添加节点即可，无需对其他分片的数据进行迁移。但是，连续分片也有可能存在数据热点的问题，就像图中按时间字段分片的例子，有些节点可能会被频繁查询压力较大，热数据节点就成为了整个集群的瓶颈。而有些节点可能存的是历史数据，很少需要被查询到。

2 随机分片其实并不是随机的，也遵循一定规则。通常，我们会采用Hash取模的方式进行分片拆分，所以有些时候也被称为离散分片。随机分片的数据相对比较均匀，不容易出现热点和并发访问的瓶颈。但是，后期分片集群扩容起来需要迁移旧的数据。使用一致性Hash算法能够很大程度的避免这个问题，所以很多中间件的分片集群都会采用一致性Hash算法。离散分片也很容易面临跨分片查询的复杂问题。

### 2.2 .3数据迁移，容量规划，扩容等问题

很少有项目会在初期就开始考虑分片设计的，一般都是在业务高速发展面临性能和存储的瓶颈时才会提前准备。因此，不可避免的就需要考虑历史数据迁移的问题。一般做法就是通过程序先读出历史数据，然后按照指定的分片规则再将数据写入到各个分片节点中。

此外，我们需要根据当前的数据量和QPS等进行容量规划，综合成本因素，推算出大概需要多少分片（一般建议单个分片上的单表数据量不要超过1000W）。

如果是采用随机分片，则需要考虑后期的扩容问题，相对会比较麻烦。如果是采用的范围分片，只需要添加节点就可以自动扩容。

### 2.2.X 美团的方案 https://tech.meituan.com/2016/11/18/dianping-order-db-sharding.html 



## 2.3 分布式全局唯一ID

1 Twitter的Snowflake（又名“雪花算法”）

 Twitter 的 Snowflake 算法是笔者近几年在分布式系统项目中使用最多的，未发现重复或并发的问题。该算法生成的是 64 位唯一 Id（由 41 位的 timestamp+ 10 位自定义的机器码 + 13 位累加计数器组成 ).

2 UUID/GUID（一般应用程序和数据库均支持）

使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。

3 MongoDB ObjectID（类似UUID的方式）

4 Ticket Server（数据库生存方式，Flickr采用的就是这种方式）

结合数据库维护一个Sequence表：此方案的思路也很简单，在数据库中建立一个Sequence表，表的结构类似于：

 ***\*CREATE\** \**TABLE\**** `***\*SEQUENCE\****` ( 

   `tablename` ***\*varchar\****(30) NOT NULL, 

​    `nextid` ***\*bigint\****(20) NOT NULL, 

​    ***\*PRIMARY\** \**KEY\**** (`tablename`) 

 ) ENGINE=InnoDB  

每当需要为某个表的新纪录生成ID时就从Sequence表中取出对应表的nextid,并将nextid的值加1后更新到数据库中以备下次使用。此方案也较简单，但缺点同样明显：由于所有插入任何都需要访问该表，该表很容易成为系统性能瓶颈，同时它也存在单点问题，一旦该表数据库失效，整个应用程序将无法工作。有人提出使用Master-Slave进行主从同步，但这也只能解决单点问题，并不能解决读写比为1:1的访问压力问题

## 2.4 分割后的sql

### 2.4.1禁止使用：

a）各种联合查询

b）子查询

c）触发器

d）用户自定义函数

e）“事务”都用的很少

原因：对数据库性能影响极大

### 2.4.2 IN查询



### 2.4.3 非Partition key的查询

基于水平分库分表，拆分策略为常用的hash法。

**端上除了partition key只有一个非partition key作为条件查询**

映射法

![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm4NrWKQic95p7XvaOOY03nna2fMH1uJ9gM50tAyzlxW0ANGsgah4oyrLQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

基因法

![img](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbueICb608XJDZq0C1ZUG2E94XwMw5PkDx2icPHfEuzoC4ww9epk30u2co9Cxoqj9w9drrngvGZyNNOw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

> 注：写入时，基因法生成user_id，如图。关于xbit基因，例如要分8张表，23=8，故x取3，即3bit基因。根据user_id查询时可直接取模路由到对应的分库或分表。
>
> 
>
> 根据user_name查询时，先通过user_name_code生成函数生成user_name_code再对其取模路由到对应的分库或分表。id生成常用snowflake算法。

**端上除了partition key不止一个非partition key作为条件查询**

映射法

![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm485zcSMZeEicibNvpZAlVeSTSlysGPiaYicwIxWbFcVQrONFhP2RfMerG4Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

冗余法

![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm4dYadXuia2KPjxwJ0GlpH9G6FWPFfGgSibDZtXo28ZkjBvVxLbTJV4bkw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

> 注：按照order_id或buyer_id查询时路由到db_o_buyer库中，按照seller_id查询时路由到db_o_seller库中。感觉有点本末倒置！有其他好的办法吗？改变技术栈呢？

**后台除了partition key还有各种非partition key组合条件查询**

NoSQL法

![img](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbueICb608XJDZq0C1ZUG2E9476Mmle7AVZNia9SWLKMuMyFciaHNemsBMo6huZ8W5zNby5ge09wxPqJQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

冗余法

![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm46bDSxkPLnV3pVON40vRO1GlZe1SqK1CSH0hcyLONgQYmd3VgO51Ong/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

 非partition key跨库跨表分页查询问题

基于水平分库分表，拆分策略为常用的hash法。

> 注：用NoSQL法解决（ES等）。



### 2.4.4 夸库分页怎么玩？

### 2.4.5. ORDER BY xxx OFFSET xxx LIMIT xxx

分库后的难题：如何确认全局偏移量

分库后传统解决方案：查询改写+内存排序

a）ORDER BY time OFFSET 0 LIMIT 10000+100

b）对20200条记录进行排序

c）返回第10000至10100条记录

优化方案一：增加辅助id，以减少查询量

优化方案二：模糊查询

a）业务上：禁止查询XX页之后的数据

b）业务上：允许模糊返回 => 第100页数据的精确性真这么重要么？

最后的大招！！！（由于时间问题，只在DTCC2015上分享了哟）

**优化方案三：终极方案，业务无损，查询改写与两段查询**

需求：ORDER BY x OFFSET 10000 LIMIT 4; 如何在分库下实现（假设分3库）

**步骤一、查询改写**： ORDER BY x OFFSET **3333** LIMIT 4

[4,7,9,10] <= 1库返回

[3,5,6,7] <= 2库返回

[6,8,9,11] <= 3库返回

**步骤二、找到步骤一返回的min和max**，即3和11

**步骤三、通过min和max二次查询**：ORDER BY x WHERE x **BETWEEN** 3 AND 11

[3,4,7,9,10] <= 1库返回，4在1库offset是3333，于是3在1库的offset是3332

[3,5,6,7,11] <= 2库返回，3在2库offset是3333

[3,5,6,8,9,11] <= 3库返回，6在3库offset是3333，于是3在3库的offset是3331

**步骤四、找出全局OFFSET**

3是全局offset3332+3333+3331=9996

当当当当，跳过3,3,3,4，于是全局OFFSET 10000 LIMIT 4是[5,5,6,6]

## 2.5 分片后的问题

### 2.5 .1跨分片join

Join是关系型数据库中最常用的特性，但是在分片集群中，join也变得非常复杂。应该尽量避免跨分片的join查询（这种场景，比上面的跨分片分页更加复杂，而且对性能的影响很大）。通常有以下几种方式来避免：

### 1.全局表

全局表的概念之前在“垂直分库”时提过。基本思想一致，就是把一些类似数据字典又可能会产生join查询的表信息放到各分片中，从而避免跨分片的join。

### 2.ER分片

在关系型数据库中，表之间往往存在一些关联的关系。如果我们可以先确定好关联关系，并将那些存在关联关系的表记录存放在同一个分片上，那么就能很好的避免跨分片join问题。在一对多关系的情况下，我们通常会选择按照数据较多的那一方进行拆分。如下图所示：

 ![æ°´å¹³ååºåè¡¨çå³é®æ­¥éª¤ä»¥åå¯è½éå°çé®é¢](https://static001.infoq.cn/resource/image/bc/3b/bc29ebe33477fedaeedac4838acd413b.jpg) 

这样一来，Data Node1上面的订单表与订单详细表就可以直接关联，进行局部的join查询了，Data Node2上也一样。基于ER分片的这种方式，能够有效避免大多数业务场景中的跨分片join问题。

### 3.内存计算

随着spark内存计算的兴起，理论上来讲，很多跨数据源的操作问题看起来似乎都能够得到解决。可以将数据丢给spark集群进行内存计算，最后将计算结果返回。

### 2.5.2 跨分片的排序分页

一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示：

 ![æ°´å¹³ååºåè¡¨çå³é®æ­¥éª¤ä»¥åå¯è½éå°çé®é¢](https://static001.infoq.cn/resource/image/2a/cc/2a7c92a59ae17c99c68b2e6d8ffee3cc.jpg) 

上面图中所描述的只是最简单的一种情况（取第一页数据），看起来对性能的影响并不大。但是，如果想取出第10页数据，情况又将变得复杂很多，如下图所示：

 ![æ°´å¹³ååºåè¡¨çå³é®æ­¥éª¤ä»¥åå¯è½éå°çé®é¢](https://static001.infoq.cn/resource/image/2a/cc/2a7c92a59ae17c99c68b2e6d8ffee3cc.jpg) 

有些读者可能并不太理解，为什么不能像获取第一页数据那样简单处理（排序取出前10条再合并、排序）。其实并不难理解，因为各分片节点中的数据可能是随机的，为了排序的准确性，必须把所有分片节点的前N页数据都排序好后做合并，最后再进行整体的排序。很显然，这样的操作是比较消耗资源的，用户越往后翻页，系统性能将会越差

### 2.5.3 跨分片的函数处理

 在使用Max、Min、Sum、Count之类的函数进行统计和计算的时候，需要先在每个分片数据源上执行相应的函数处理，然后再将各个结果集进行二次处理，最终再将处理结果返回。如下图所示： 

 ![æ°´å¹³ååºåè¡¨çå³é®æ­¥éª¤ä»¥åå¯è½éå°çé®é¢](https://static001.infoq.cn/resource/image/e0/09/e054a401d37415e90687e5cb4ebe1009.jpg) 

### 2.5.4 跨分片事务问题

### 2.5.5 扩容问题

**连续分片**扩容很简单，但是基于水平分库分表，拆分策略为常用的hash法。

**水平扩容库（升级从库法）**

**![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm4CObBX3TiaLnJDvGp8xe0q0arAbgz4A8PpXqLhgicVopbyyBHyfCxaCJQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)**

> 注：扩容是成倍的。

**水平扩容表（双写迁移法）**

**![img](https://mmbiz.qpic.cn/mmbiz_png/qibDtCQTAY1CJXhCdWfn4Qp67wFfoJcm4YEH6MHuicvf3icEMXIcHZNgbibnw5VkMZCjKJHEac7TBP0KKxJ0arT7eQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)**

- 第一步：（同步双写）修改应用配置和代码，加上双写，部署；
- 第二步：（同步双写）将老库中的老数据复制到新库中；
- 第三步：（同步双写）以老库为准校对新库中的老数据；
- 第四步：（同步双写）修改应用配置和代码，去掉双写，部署；

> 注：双写是通用方案。







# 数据库之父Codd的12条法则



另外，我们回顾一下数据库之父Codd的12条法则，作为数据库设计的指导性方针：

1. **信息法则 
   **关系数据库中的所有信息都用唯一的一种方式表示——表中的值。
2. **保证访问法则 
   **依靠表名、主键值和列名的组合，保证能访问每个数据项。
3. **空值的系统化处理 
   **支持空值（NULL），以系统化的方式处理空值，空值不依赖于数据类型。
4. **基于关系模型的动态联机目录 
   **数据库的描述应该是自描述的，在逻辑级别上和普通数据采用同样的表示方式，即数据库必须含有描述该数据库结构的系统表或者数据库描述信息应该包含在用户可以访问的表中。
5. **统一的数据子语言法则 
   **一个关系数据库系统可以支持几种语言和多种终端使用方式，但必须至少有一种语言，它的语句能够一某种定义良好的语法表示为字符串，并能全面地支持以下所有规则：数据定义、视图定义、数据操作、约束、授权以及事务。（这种语言就是SQL)
6. **视图更新法则 
   **所有理论上可以更新的视图也可以由系统更新。
7. **高级的插入、更新和删除操作 
   **把一个基础关系或派生关系作为单个操作对象处理的能力不仅适应于数据的检索，还适用于数据的插入、修改个删除，即在插入、修改和删除操作中数据行被视作集合。
8. **数据的物理独立性 
   **不管数据库的数据在存储表示或访问方式上怎么变化，应用程序和终端活动都保持着逻辑上的不变性。
9. **数据的逻辑独立性 
   **当对表做了理论上不会损害信息的改变时，应用程序和终端活动都会保持逻辑上的不变性。
10. **数据完整性的独立性 
    **专用于某个关系型数据库的完整性约束必须可以用关系数据库子语言定义，而且可以存储在数据目录中，而非程序中。
11. **分布独立性 
    **不管数据在物理是否分布式存储，或者任何时候改变分布策略，RDBMS的数据操纵子语言必须能使应用程序和终端活动保持逻辑上的不变性。
12. **非破坏性法则 
    **如果一个关系数据库系统支持某种低级（一次处理单个记录）语言，那么这个低级语言不能违反或绕过更高级语言（一次处理多个记录）规定的完整性法则或约束，即用户不能以任何方式违反数据库的约束。





